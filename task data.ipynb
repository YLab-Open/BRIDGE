{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b845c260",
   "metadata": {},
   "source": [
    "## Resource "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822cc371",
   "metadata": {},
   "source": [
    "**Paper BRIDGE**: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text (https://arxiv.org/abs/2504.19467)\n",
    "\n",
    "**Github repo**: BRIDGE (https://github.com/YLab-Open/BRIDGE)\n",
    "\n",
    "**Dataset**: BRIDGE-Open (https://huggingface.co/datasets/YLab-Open/BRIDGE-Open)\n",
    "\n",
    "**Leaderboards**: BRIDGE-Medical-Leaderboard (https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc10cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a8f5e7",
   "metadata": {},
   "source": [
    "## Download and organize files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c8386",
   "metadata": {},
   "source": [
    "(Optional) A. Download the dataset from Hugging Face via python script\n",
    "\n",
    "See dataset_download.py for more details.\n",
    "\n",
    "(Requirement: pip install huggingface_hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f864eb",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bba923",
   "metadata": {},
   "source": [
    "(Optional) B. Manually Download the dataset from Hugging Face\n",
    "\n",
    "1. Web: https://huggingface.co/datasets/YLab-Open/BRIDGE-Open/tree/main\n",
    "\n",
    "2. Manually download **\"Dataset.zip\"** and **\"Examples.zip\"** files\n",
    "\n",
    "3. Extract them to the **\"dataset_raw\"** directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef5775",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbd5e4",
   "metadata": {},
   "source": [
    "Finally, the directory structure should look like this:\n",
    "```\n",
    "dataset_raw/\n",
    "├── task_1.SFT.json\n",
    "├── task_1.SFT.json\n",
    "└── example/\n",
    "    ├── task_1.example.json\n",
    "    ├── task_1.example.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448102cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "504df7f6",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ba2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05c0a3",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks(root: str | Path = \"dataset_raw\") -> dict[str, dict[str, Any]]:\n",
    "    root = Path(root)\n",
    "    \n",
    "    def read_json(path: Path) -> Any:\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    data_files    = {p.name.split(\".SFT\", 1)[0]: p for p in root.glob(\"*.SFT.json\")}\n",
    "    example_files = {p.name.split(\".example\", 1)[0]: p for p in (root / \"example\").glob(\"*.example.json\")}\n",
    "\n",
    "    assert set(data_files.keys()) == set(example_files.keys()), \\\n",
    "        \"Data and example files must match in task names.\"\n",
    "\n",
    "    dict_task_data: dict[str, dict[str, Any]] = {}\n",
    "\n",
    "    for task in data_files:\n",
    "        dict_task_data[task] = {\n",
    "            \"example\": read_json(example_files[task]),\n",
    "            \"test\":    read_json(data_files[task]),\n",
    "        }\n",
    "        dict_task_data[task][\"corpus\"] = [ data['input'] for data in dict_task_data[task][\"example\"] ] + [ data['input'] for data in dict_task_data[task][\"test\"] ]\n",
    "\n",
    "    return dict_task_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tasks from the dataset folder\n",
    "path_dir_data = \"dataset_raw\"\n",
    "dict_task_data = load_tasks(root=path_dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e9ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb27547d",
   "metadata": {},
   "source": [
    "## Explore data for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacceef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: get one specific task\n",
    "task_name = \"BrainMRI-AIS\"\n",
    "task_data = dict_task_data[task_name]\n",
    "print(f\"Task: {task_name}\")\n",
    "print(f\"Number of 'examples': {len(task_data['example'])}\")\n",
    "print(f\"Number of 'test' cases: {len(task_data['test'])}\")\n",
    "print(f\"Number of 'corpus' entries: {len(task_data['corpus'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the corpus\n",
    "idx = 1\n",
    "example_one = task_data[\"corpus\"][idx]\n",
    "print(f\"Corpus:\\n{example_one}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A whole data from test\n",
    "idx = 1\n",
    "test_one = task_data[\"test\"][idx]\n",
    "for key, value in test_one.items():\n",
    "    print(f\"{key}: {value.strip() if isinstance(value, str) else value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A whole data from example\n",
    "idx = 1\n",
    "example_one = task_data[\"example\"][idx]\n",
    "for key, value in example_one.items():\n",
    "    print(f\"{key}: {value.strip() if isinstance(value, str) else value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dce674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74e0605a",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ab5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e904c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
